{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv file\n",
    "\n",
    "Text file is not properly delimited. Hence, need to write custom reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    with open(file_name) as f:\n",
    "        line = f.readlines()        \n",
    "    # get length of the list\n",
    "    #print len(line) - 1\n",
    "    # skip header row\n",
    "    return line[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the last word out based on ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_csv(data, delimiter=','):\n",
    "    clean_data = []\n",
    "    clean_labels = []\n",
    "    for line in data:\n",
    "        # split the line based on delimiter\n",
    "        words = line.split(delimiter)\n",
    "        # join words till the last word\n",
    "        data_line = ' '.join(words[:-1])\n",
    "        # append to list\n",
    "        if (data_line != ''):\n",
    "            clean_data.append( re.sub(\"[^a-zA-Z ']\",\"\",data_line.lower()) )\n",
    "            clean_labels.append( words[-1][:-1].lower() )\n",
    "    return np.array( clean_data ),np.array( clean_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2761 2761\n",
      "['ship shape and bristol fashion' 'know the ropes' 'graveyard shift' ...,\n",
      " 'gofaster' 'red tape' 'in a pickle'] ['english' 'english' 'english' ..., 'english' 'english' 'english']\n"
     ]
    }
   ],
   "source": [
    "# get the data dump\n",
    "data = read_csv('../lang_data.csv')\n",
    "clean_data, clean_labels = clean_csv(data)\n",
    "print clean_data.size, clean_labels.size\n",
    "print clean_data, clean_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random split for training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/workspace/virtualenv/dprophet/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# 70% train, 15% validation, 15% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(clean_data, clean_labels, test_size=0.3, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1932 414 415 1932 414 415\n"
     ]
    }
   ],
   "source": [
    "print x_train.size, x_test.size, x_val.size, y_train.size, y_test.size, y_val.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Bag of words Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n"
     ]
    }
   ],
   "source": [
    "print \"Creating the bag of words...\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create vectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 1500)\n",
    "\n",
    "# Use fit transform to build the Bag-of-words\n",
    "train_data_features = vectorizer.fit_transform(x_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1932, 1500)\n"
     ]
    }
   ],
   "source": [
    "print train_data_features.shape\n",
    "## Take a look at the words in the vocabulary\n",
    "##vocab = vectorizer.get_feature_names()\n",
    "##print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.8599033816 %\n"
     ]
    }
   ],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(x_test).toarray()\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "n = np.count_nonzero(result == y_test)\n",
    "print (np.float32(n)*100/result.size),\"%\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
